"""Item generator service for LLM-based assessment item generation.

This service generates assessment items from skill blueprints using the LLM.
The LLM generates BOTH the question content AND the correct answer.
The backend stores and verifies - it does NOT compute answers.
"""

import json
import logging
import random
from typing import Any

from application.agents.llm_provider import LlmMessage, LlmProvider
from application.services.blueprint_store import BlueprintStore
from domain.models.blueprint_models import DifficultyLevel, ItemType, Skill
from domain.models.generated_item import GeneratedItem

logger = logging.getLogger(__name__)


# =============================================================================
# Item Generator System Prompt
# =============================================================================

ITEM_GENERATOR_SYSTEM_PROMPT = """You are an expert assessment item writer for educational and professional certification exams.

Your job is to generate high-quality assessment items from skill blueprints.

## Requirements

1. ACCURACY: The correct answer you provide MUST be mathematically/factually correct.
   - For math: compute the actual result accurately
   - For networking: apply the actual rules (subnetting, CIDR, etc.)
   - Verify your answer before responding

2. CLARITY: The stem (question) should be unambiguous. One and only one answer is correct.

3. DISTRACTORS: Wrong options should be:
   - Plausible (represent common errors students make)
   - Clearly distinguishable from each other
   - NOT trick questions or edge cases
   - Based on the distractor strategies provided

4. DIFFICULTY: Follow the difficulty constraints provided:
   - Easy: Straightforward application, smaller numbers, obvious patterns
   - Medium: Requires multi-step thinking, moderate complexity
   - Hard: Edge cases, larger numbers, more complex scenarios

5. EXPLANATION: Provide a clear, educational explanation that could help a learner understand why the answer is correct.

## Output Format

You MUST respond with valid JSON only. No markdown, no explanations outside JSON.

{
  "stem": "The question text",
  "options": ["Option A", "Option B", "Option C", "Option D"],
  "correct_answer": "The exact correct value (must match one of the options)",
  "correct_index": 0,
  "explanation": "Educational explanation of why this is correct..."
}

IMPORTANT:
- options must have exactly the number requested (usually 4)
- correct_answer must EXACTLY match one of the options
- correct_index must be the 0-based index of the correct answer in options
- Shuffle the position of the correct answer (don't always put it first)
"""


class ItemGeneratorError(Exception):
    """Raised when item generation fails."""

    pass


class ItemGeneratorService:
    """Service for generating assessment items from skill blueprints.

    This service:
    1. Loads the skill blueprint
    2. Builds a generation prompt for the LLM
    3. Calls the LLM to generate item content + answer + distractors
    4. Validates the LLM output
    5. Returns a GeneratedItem ready for presentation

    The correct answer is generated BY THE LLM, not computed by the backend.
    This allows for flexibility in question types while ensuring accuracy
    through the LLM's reasoning.
    """

    def __init__(
        self,
        blueprint_store: BlueprintStore,
        llm_provider: LlmProvider,
    ):
        """Initialize the item generator.

        Args:
            blueprint_store: Service for loading blueprints
            llm_provider: The LLM provider for generation
        """
        self._blueprint_store = blueprint_store
        self._llm = llm_provider

    async def generate_item(
        self,
        skill_id: str,
        domain_id: str,
        sequence_number: int,
        difficulty_level: DifficultyLevel = DifficultyLevel.MEDIUM,
    ) -> GeneratedItem:
        """Generate an assessment item from a skill blueprint.

        Args:
            skill_id: The skill blueprint to use
            domain_id: The exam domain this item belongs to
            sequence_number: Position in the session (1-indexed)
            difficulty_level: Target difficulty

        Returns:
            A GeneratedItem with content and correct answer

        Raises:
            ItemGeneratorError: If generation fails
        """
        # 1. Load skill blueprint
        try:
            skill = await self._blueprint_store.get_skill(skill_id)
        except Exception as e:
            raise ItemGeneratorError(f"Failed to load skill {skill_id}: {e}") from e

        # 2. Build generation prompt
        prompt = self._build_generation_prompt(skill, difficulty_level)

        # 3. Call LLM to generate
        try:
            llm_response = await self._call_llm_for_generation(prompt)
        except Exception as e:
            raise ItemGeneratorError(f"LLM generation failed: {e}") from e

        # 4. Parse and validate response
        try:
            parsed = self._parse_llm_response(llm_response, skill)
        except Exception as e:
            raise ItemGeneratorError(f"Failed to parse LLM response: {e}") from e

        # 5. Create GeneratedItem
        difficulty_config = skill.get_difficulty_config(difficulty_level)

        item = GeneratedItem.create(
            skill_id=skill_id,
            domain_id=domain_id,
            sequence_number=sequence_number,
            difficulty_level=difficulty_level,
            difficulty_value=difficulty_config.value,
            item_type=skill.item_type,
            stem=parsed["stem"],
            correct_answer=parsed["correct_answer"],
            options=parsed.get("options", []),
            correct_index=parsed.get("correct_index"),
            explanation=parsed.get("explanation", ""),
            evaluation_method=skill.evaluation_method,
            options_shuffled=True,  # LLM shuffles
        )

        logger.info(f"Generated item {item.id} for skill {skill_id}, difficulty {difficulty_level.value}")
        return item

    def _build_generation_prompt(self, skill: Skill, difficulty_level: DifficultyLevel) -> str:
        """Build the prompt for item generation.

        Args:
            skill: The skill blueprint
            difficulty_level: Target difficulty

        Returns:
            The generation prompt string
        """
        difficulty_config = skill.get_difficulty_config(difficulty_level)

        # Build stem templates section
        stem_templates_text = ""
        if skill.stem_templates:
            stem_templates_text = "Example question formats:\n" + "\n".join(f"- {t}" for t in skill.stem_templates)

        # Build distractor strategies section
        distractor_text = ""
        if skill.distractor_strategies:
            distractor_text = "Distractor strategies (use these to create plausible wrong answers):\n"
            for strategy in skill.distractor_strategies:
                distractor_text += f"- {strategy.type}: {strategy.description}\n"

        # Build constraints section
        constraints_text = ""
        if difficulty_config.constraints:
            constraints_text = f"Constraints for {difficulty_level.value} difficulty:\n"
            for constraint in difficulty_config.constraints:
                constraints_text += f"- {constraint}\n"

        prompt = f"""Generate a {difficulty_level.value} difficulty assessment item for this skill:

SKILL: {skill.name}
DESCRIPTION: {skill.description}
DOMAIN: {skill.domain}
TOPIC: {skill.topic}
ITEM TYPE: {skill.item_type.value}
NUMBER OF OPTIONS: {skill.option_count}

{stem_templates_text}

{constraints_text}

{distractor_text}

Generate a complete item with:
1. A clear, unambiguous question (stem)
2. The correct answer (you compute/determine it)
3. {skill.option_count - 1} plausible but incorrect options (distractors)
4. A brief explanation of WHY the answer is correct

Remember:
- The correct answer must be accurate
- Place the correct answer at a random position in the options
- Each option must be distinct
- For math: show actual computed values, not formulas
- For networking: use proper notation (e.g., 192.168.1.0/24)

Respond with JSON only."""

        return prompt

    async def _call_llm_for_generation(self, prompt: str) -> str:
        """Call the LLM to generate an item.

        Args:
            prompt: The generation prompt

        Returns:
            The LLM's response text
        """
        messages = [
            LlmMessage.system(ITEM_GENERATOR_SYSTEM_PROMPT),
            LlmMessage.user(prompt),
        ]

        # Use non-streaming for item generation
        response = await self._llm.chat(messages=messages, tools=None)

        return response.content

    def _parse_llm_response(self, response: str, skill: Skill) -> dict[str, Any]:
        """Parse and validate the LLM's response.

        Args:
            response: The raw LLM response
            skill: The skill blueprint for validation

        Returns:
            Parsed item data

        Raises:
            ValueError: If response is invalid
        """
        # Try to extract JSON from response
        response = response.strip()

        # Handle markdown code blocks
        if response.startswith("```"):
            lines = response.split("\n")
            # Find start and end of code block
            start_idx = 1 if lines[0].startswith("```") else 0
            end_idx = len(lines)
            for i in range(len(lines) - 1, -1, -1):
                if lines[i].strip() == "```":
                    end_idx = i
                    break
            response = "\n".join(lines[start_idx:end_idx])

        try:
            data = json.loads(response)
        except json.JSONDecodeError as e:
            logger.error(f"Failed to parse LLM response as JSON: {response[:500]}")
            raise ValueError(f"Invalid JSON in LLM response: {e}") from e

        # Validate required fields
        required_fields = ["stem", "correct_answer"]
        if skill.item_type == ItemType.MULTIPLE_CHOICE:
            required_fields.extend(["options", "correct_index"])

        for field in required_fields:
            if field not in data:
                raise ValueError(f"Missing required field: {field}")

        # Validate options count for multiple choice
        if skill.item_type == ItemType.MULTIPLE_CHOICE:
            if len(data["options"]) != skill.option_count:
                logger.warning(f"Expected {skill.option_count} options, got {len(data['options'])}")
                # Try to fix by padding or trimming
                while len(data["options"]) < skill.option_count:
                    data["options"].append(f"Option {len(data['options']) + 1}")
                data["options"] = data["options"][: skill.option_count]

            # Validate correct_index
            if not 0 <= data["correct_index"] < len(data["options"]):
                # Try to find correct answer in options
                try:
                    data["correct_index"] = data["options"].index(data["correct_answer"])
                except ValueError:
                    logger.warning("correct_answer not found in options, using index 0")
                    data["correct_index"] = 0
                    data["options"][0] = data["correct_answer"]

            # Ensure correct_answer matches the option at correct_index
            if data["options"][data["correct_index"]] != data["correct_answer"]:
                logger.warning("correct_answer doesn't match options[correct_index], fixing...")
                data["options"][data["correct_index"]] = data["correct_answer"]

        return data

    async def generate_items_for_plan(
        self,
        item_plan: list[dict[str, Any]],
    ) -> list[GeneratedItem]:
        """Generate multiple items according to a plan.

        Args:
            item_plan: List of plan entries with skill_id, domain_id, difficulty_level, sequence_number

        Returns:
            List of generated items
        """
        items = []
        for entry in item_plan:
            try:
                item = await self.generate_item(
                    skill_id=entry["skill_id"],
                    domain_id=entry["domain_id"],
                    sequence_number=entry["sequence_number"],
                    difficulty_level=DifficultyLevel(entry.get("difficulty_level", "medium")),
                )
                items.append(item)
            except ItemGeneratorError as e:
                logger.error(f"Failed to generate item for plan entry {entry}: {e}")
                # Continue with other items
                continue

        return items


def select_random_difficulty(
    distribution: dict[DifficultyLevel, int],
    remaining: dict[DifficultyLevel, int] | None = None,
) -> DifficultyLevel:
    """Select a random difficulty level based on distribution.

    Args:
        distribution: Target count per difficulty level
        remaining: Remaining counts (for tracking during generation)

    Returns:
        Selected difficulty level
    """
    if remaining is None:
        remaining = distribution.copy()

    # Filter to levels with remaining capacity
    available = {level: count for level, count in remaining.items() if count > 0}

    if not available:
        # Fall back to any available
        return random.choice(list(DifficultyLevel))  # nosec B311

    # Weighted random selection
    levels = list(available.keys())
    weights = list(available.values())
    return random.choices(levels, weights=weights, k=1)[0]  # nosec B311
